import sqlite3
import pandas as pd
import numpy as np
import requests
import re
import json
import threading
import unicodedata
import random
from typing import Tuple, Dict, Any, Optional, Generator
import difflib

ARRONDISSEMENT_MAP = {
    # 1er
    "1": 1, "1er": 1, "1e": 1, "1eme": 1, "1√®me": 1, "1ieme": 1, "1i√©me": 1,
    "premier": 1, "paris 1er": 1, "paris 1e": 1,

    # 2e
    "2": 2, "2e": 2, "2eme": 2, "2√®me": 2, "2ieme": 2, "2i√©me": 2,
    "deuxieme": 2, "deuxi√®me": 2, "paris 2e": 2,

    # 3e
    "3": 3, "3e": 3, "3eme": 3, "3√®me": 3, "3ieme": 3, "3i√©me": 3,
    "troisieme": 3, "troisi√®me": 3, "paris 3e": 3,

    # 4e
    "4": 4, "4e": 4, "4eme": 4, "4√®me": 4, "4ieme": 4, "4i√©me": 4,
    "quatrieme": 4, "quatri√®me": 4, "paris 4e": 4,

    # 5e
    "5": 5, "5e": 5, "5eme": 5, "5√®me": 5, "5ieme": 5, "5i√©me": 5,
    "cinquieme": 5, "cinqui√®me": 5, "paris 5e": 5,

    # 6e
    "6": 6, "6e": 6, "6eme": 6, "6√®me": 6, "6ieme": 6, "6i√©me": 6,
    "sixieme": 6, "sixi√®me": 6, "paris 6e": 6,

    # 7e
    "7": 7, "7e": 7, "7eme": 7, "7√®me": 7, "7ieme": 7, "7i√©me": 7,
    "septieme": 7, "septi√®me": 7, "paris 7e": 7,

    # 8e
    "8": 8, "8e": 8, "8eme": 8, "8√®me": 8, "8ieme": 8, "8i√©me": 8,
    "huitieme": 8, "huiti√®me": 8, "paris 8e": 8,

    # 9e
    "9": 9, "9e": 9, "9eme": 9, "9√®me": 9, "9ieme": 9, "9i√©me": 9,
    "neuvieme": 9, "neuvi√®me": 9, "paris 9e": 9,

    # 10e
    "10": 10, "10e": 10, "10eme": 10, "10√®me": 10, "10ieme": 10, "10i√©me": 10,
    "dixieme": 10, "dixi√®me": 10, "paris 10e": 10,

    # 11e
    "11": 11, "11e": 11, "11eme": 11, "11√®me": 11, "11ieme": 11, "11i√©me": 11,
    "onzieme": 11, "onzi√®me": 11, "paris 11e": 11,

    # 12e
    "12": 12, "12e": 12, "12eme": 12, "12√®me": 12, "12ieme": 12, "12i√©me": 12,
    "douzieme": 12, "douzi√®me": 12, "paris 12e": 12,

    # 13e
    "13": 13, "13e": 13, "13eme": 13, "13√®me": 13, "13ieme": 13, "13i√©me": 13,
    "treizieme": 13, "treizi√®me": 13, "paris 13e": 13,

    # 14e
    "14": 14, "14e": 14, "14eme": 14, "14√®me": 14, "14ieme": 14, "14i√©me": 14,
    "quatorzieme": 14, "quatorzi√®me": 14, "paris 14e": 14,

    # 15e
    "15": 15, "15e": 15, "15eme": 15, "15√®me": 15, "15ieme": 15, "15i√©me": 15,
    "quinzieme": 15, "quinzi√®me": 15, "paris 15e": 15,

    # 16e
    "16": 16, "16e": 16, "16eme": 16, "16√®me": 16, "16ieme": 16, "16i√©me": 16,
    "seizieme": 16, "seizi√®me": 16, "paris 16e": 16,

    # 17e
    "17": 17, "17e": 17, "17eme": 17, "17√®me": 17, "17ieme": 17, "17i√©me": 17,
    "dix-septieme": 17, "dix-septi√®me": 17, "paris 17e": 17,

    # 18e
    "18": 18, "18e": 18, "18eme": 18, "18√®me": 18, "18ieme": 18, "18i√©me": 18,
    "dix-huitieme": 18, "dix-huiti√®me": 18, "paris 18e": 18,

    # 19e
    "19": 19, "19e": 19, "19eme": 19, "19√®me": 19, "19ieme": 19, "19i√©me": 19,
    "dix-neuvieme": 19, "dix-neuvi√®me": 19, "paris 19e": 19,

    # 20e
    "20": 20, "20e": 20, "20eme": 20, "20√®me": 20, "20ieme": 20, "20i√©me": 20,
    "vingtieme": 20, "vingti√®me": 20, "paris 20e": 20
}
STATUS_MAP = {
    # FIN
    "fin": "FIN", "fini": "FIN", "finis": "FIN", "termine": "FIN", "termin√©": "FIN",
    "termin√©e": "FIN", "acheve": "FIN", "achev√©": "FIN", "achev√©e": "FIN",
    "clos": "FIN", "ferm√©": "FIN", "finalise": "FIN", "finalis√©": "FIN", "finalis√©e": "FIN",

    # LIVRAISON
    "livraison": "LIVRAISON", "livre": "LIVRAISON", "livr√©": "LIVRAISON", "livr√©e": "LIVRAISON",
    "remis": "LIVRAISON", "remise": "LIVRAISON",

    # ETUDES
    "etude": "ETUDES", "etudes": "ETUDES", "etudier": "ETUDES", "analyse": "ETUDES",
    "conception": "ETUDES", "planification": "ETUDES",

    # TRAVAUX
    "travaux": "TRAVAUX", "construction": "TRAVAUX", "chantier": "TRAVAUX",
    "batir": "TRAVAUX", "batiment": "TRAVAUX", "r√©alisation": "TRAVAUX",

    # NON DEMARRE
    "non demarre": "NON DEMARRE", "pas commence": "NON DEMARRE", "pas commenc√©": "NON DEMARRE",
    "non commenc√©": "NON DEMARRE", "non debut√©": "NON DEMARRE", "non d√©but√©": "NON DEMARRE",

    # PROCEDURES
    "procedure": "PROCEDURES", "procedures": "PROCEDURES", "administratif": "PROCEDURES",
    "juridique": "PROCEDURES", "autorisation": "PROCEDURES", "validation": "PROCEDURES",

    # ABANDONN√â
    "abandonne": "ABANDONN√â", "abandonn√©": "ABANDONN√â", "abandonn√©e": "ABANDONN√â",
    "annule": "ABANDONN√â", "annul√©": "ABANDONN√â", "annul√©e": "ABANDONN√â",
    "suspendu": "ABANDONN√â", "suspendue": "ABANDONN√â"
}
SEEDS = {
    "Pr√©vention et s√©curit√©": [
        "s√©curit√©","s√©curisation","danger","risque","√©clairage","lampadaire","luminaire",
        "cam√©ra","vid√©osurveillance","police","incendie","pompier","signalisation",
        "passage pi√©ton","radar","alarme","sir√®ne","feu tricolore","ralentisseur","dos d‚Äô√¢ne"
    ],
    "Cadre de vie": [
        "cadre de vie","qualit√© de vie","espaces publics","urbanisme","am√©nagement","place",
        "square","mobilier urbain","banc","aire de repos","aire de jeux","parvis","esplanade",
        "r√©novation","embellissement","poteau","fontaine","kiosque","terrasse","pergola"
    ],
    "Environnement": [
        "environnement","nature","biodiversit√©","jardin","parc public","√©cologie","arbre","arbres",
        "plantation","verdissement","compost","eau","toiture v√©g√©tale","mur v√©g√©tal","v√©g√©talisation",
        "climat","d√©veloppement durable","recyclage","pollution","air","√©nergie renouvelable",
        "solaire","photovolta√Øque","prairie fleurie","nichoir","h√¥tel √† insectes"
    ],
    "Sport": [
        "sport","football","foot","gymnase","terrain","stade","basket","handball","tennis",
        "natation","piscine","running","course","dojo","arts martiaux","rugby","volley",
        "badminton","patinoire","skatepark","escalade","fitness","musculation","city stade"
    ],
    "Culture et patrimoine": [
        "culture","patrimoine","biblioth√®que","m√©diath√®que","mus√©e","spectacle","archive","exposition",
        "art","cin√©ma","festival","th√©√¢tre","concert","musique","danse","peinture","sculpture",
        "parc d‚Äôattraction","parc de loisirs","divertissement","galerie","salle de spectacle",
        "conservatoire"
    ],
    "Solidarit√©s": [
        "solidarit√©","inclusion","entraide","accessibilit√©","handicap","aide alimentaire",
        "√©picerie solidaire","r√©fugi√©s","personnes √¢g√©es","soutien","logement social","sans-abri",
        "pr√©carit√©","accueil de jour"
    ],
    "Education et jeunesse": [
        "√©ducation","√©cole","cr√®che","jeunesse","coll√®ge","lyc√©e","universit√©","activit√© √©ducative",
        "p√©riscolaire","bibliobus","formation","alphab√©tisation","enseignement","cantine",
        "soutien scolaire"
    ],
    "Mobilit√©s": [
        "mobilit√©","transport","v√©lo","piste cyclable","trottoir","stationnement","parking","bus",
        "tram","m√©tro","voie verte","circulation","pi√©ton","pi√©tonisation","route","chauss√©e",
        "signalisation routi√®re","borne de recharge","recharge √©lectrique"
    ],
    "Propret√©": [
        "propret√©","d√©chet","d√©chets","nettoyage","sensibilisation","tri","poubelles","corbeilles",
        "d√©p√¥ts sauvages","ramassage","balayage","ordures","collecte","benne","lavage voirie"
    ],
    "Sant√©": [
        "sant√©","soin","h√¥pital","pr√©vention sanitaire","centre de sant√©","clinique","pharmacie",
        "vaccination","m√©decin","consultation","urgences","sant√© publique","infirmerie","cabinet m√©dical"
    ],
    "Attractivit√© et emploi": [
        "emploi","travail","commerce","attractivit√©","√©conomie","artisanat","march√©","tourisme",
        "entrepreneuriat","incubateur","startup","innovation","industrie","foire","boutique",
        "attraction touristique","p√¥le √©conomique"
    ]
}

# utils.py
# Complete utility module WITHOUT any map declarations (ARRONDISSEMENT_MAP, STATUS_MAP, SEEDS, THEMATIQUE_MAP).
# These maps are assumed to be defined elsewhere in your project.


import hashlib
import time

# -----------------------
# Configuration LLM / DB
# -----------------------
OLLAMA_URL = "http://127.0.0.1:11434/api/generate"
DEFAULT_MODEL = "phi3:mini"

# Cache en m√©moire pour suggestions
_SUGGESTIONS_CACHE: Dict[str, str] = {}

# -----------------------
# SQL helpers
# -----------------------
def load_dataframe_from_sql(db_path: str, table_name: str) -> pd.DataFrame:
    """
    Charge un DataFrame depuis une base SQLite. Conserve les colonnes utiles si pr√©sentes.
    """
    conn = sqlite3.connect(db_path)
    try:
        df = pd.read_sql_query(f"SELECT * FROM {table_name}", conn)
    finally:
        conn.close()

    features = [
        "titre_projet_gagnant",
        "thematique",
        "arrondissement_projet_gagnant",
        "budget_global_projet_gagnant",
        "avancement_projet",
        "edition",
    ]
    existing = [c for c in features if c in df.columns]
    return df[existing] if existing else df

# -----------------------
# Helpers de normalisation
# -----------------------
def normalize_text(s: Any) -> str:
    s = "" if s is None else str(s)
    s = s.lower().strip()
    s = unicodedata.normalize("NFKD", s)
    s = "".join(ch for ch in s if not unicodedata.combining(ch))
    s = re.sub(r"[‚Äô'`]", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s
def variants(term: str) -> set:
    """
    Generate normalized variants of a seed term (dash/apostrophe/space forms, plural heuristic).
    Relies on normalize_text already defined in utils.py.
    """
    t = normalize_text(term)
    forms = {t, t.replace("-", " "), t.replace("‚Äô", " "), t.replace("'", " ")}
    if " " not in t and not t.endswith("s"):
        forms.add(t + "s")
    return forms

def build_thematique_map_from_seeds(seeds: dict) -> dict:
    """
    Build a normalized THEMATIQUE_MAP from SEEDS:
    { "Theme name": ["seed1", "seed2", ...], ... } -> { "normalized_term": "Theme name", ... }
    Does not declare or modify SEEDS/THEMATIQUE_MAP here; just returns the map.
    """
    themap = {}
    for theme, terms in (seeds or {}).items():
        for term in terms:
            for v in variants(term):
                themap[normalize_text(v)] = theme
    return themap
THEMATIQUE_MAP = build_thematique_map_from_seeds(SEEDS)


# -----------------------
# Extraction arrondissement / budget / th√©matique
# -----------------------

def normalize_arrondissement_from_text(text: str) -> Optional[int]:
    txt = normalize_text(text)

    # Regex for "7e arrondissement", "arr 7", "7"
    m = re.search(r"\b(\d{1,2})(?:\s*(?:e|eme|√®me|ieme|i√©me))?\s*(?:arrondissement|arr|arrond)?\b", txt)
    if m:
        num = int(m.group(1))
        if 1 <= num <= 20:
            return num

    # Fuzzy check for misspelled "arrondissement"
    words = txt.split()
    for w in words:
        close = difflib.get_close_matches(w, ["arrondissement", "arrondissment", "arrond", "arr"], n=1, cutoff=0.75)
        if close:
            # If a number is nearby in the text, extract it
            m2 = re.search(r"\b(\d{1,2})\b", txt)
            if m2:
                num = int(m2.group(1))
                if 1 <= num <= 20:
                    return num

    # Fallback via map textuelle si disponible
    try:
        for k, v in ARRONDISSEMENT_MAP.items():
            if k in txt:
                return v
    except NameError:
        pass

    return None


def parse_budget_from_text(text: str) -> Optional[float]:
    original = "" if text is None else str(text)
    txt = normalize_text(original)
    arr = normalize_arrondissement_from_text(original)

    # Liste des variantes attendues du mot "budget"
    budget_tokens = ["budget", "budjet", "bugdet", "budg√©t", "budg"]

    # V√©rifier si un mot proche de "budget" est pr√©sent
    words = txt.split()
    has_budget = any(
        difflib.get_close_matches(w, budget_tokens, n=1, cutoff=0.75)
        for w in words
    )

    # 1) "budget: 400000" / "budget 400k"
    if has_budget:
        mb = re.search(r"(?:budget|budjet|bugdet|budg)(?:\s+de)?\s*:?\s*(\d[\d\s.,]*)(\s*k)?", txt)
        if mb:
            raw = re.sub(r"[^\d]", "", mb.group(1))
            if raw:
                val = int(raw)
                if mb.group(2):
                    val *= 1000
                return float(val)

    # 2) "400 000 ‚Ç¨" / "400k eur"
    md = re.search(r"(\d[\d\s.,]*)(\s*k)?\s*(‚Ç¨|eur|euros|\$|usd)", txt)
    if md:
        raw = re.sub(r"[^\d]", "", md.group(1))
        if raw:
            val = int(raw)
            if md.group(2):
                val *= 1000
            return float(val)

    # 3) "400k"
    mk = re.search(r"\b(\d{1,3})\s*k\b", txt)
    if mk:
        return float(int(mk.group(1)) * 1000)

    # 4) Fallback: plus grand nombre significatif, exclut l'arrondissement
    nums = [int(re.sub(r"[^\d]", "", n)) for n in re.findall(r"\b\d[\d\s.,]*\b", original)]
    nums = [n for n in nums if n >= 1000 and n != arr]
    return float(max(nums)) if nums else None

def to_float_budget(b: Any) -> float:
    try:
        if pd.isna(b):
            return np.nan
    except Exception:
        pass
    try:
        return float(str(b).replace(" ", "").replace(",", "."))
    except Exception:
        return np.nan


def map_thematique_free_text(text: str, return_debug: bool = False):
    txt = normalize_text(text)
    scores = {}
    matched = {}
    try:
        keys = sorted(THEMATIQUE_MAP.keys(), key=lambda x: -len(x))
    except NameError:
        keys = []

    # Exact matches first
    for k in keys:
        if k in txt:
            pos = txt.find(k)
            score = 1
            if pos != -1 and pos < len(txt) / 2:
                score += 1
            if len(k.split()) > 1:
                score += 3
            theme = THEMATIQUE_MAP[k]
            scores[theme] = scores.get(theme, 0) + score
            matched.setdefault(theme, []).append((k, score))

    # Fuzzy matching if no exact match
    if not scores and keys:
        words = txt.split()
        for w in words:
            # find close matches among keys
            close = difflib.get_close_matches(w, keys, n=1, cutoff=0.75)
            if close:
                k = close[0]
                theme = THEMATIQUE_MAP[k]
                score = 1
                scores[theme] = scores.get(theme, 0) + score
                matched.setdefault(theme, []).append((k, score))

    if not scores:
        return ("th√©matique non pr√©cis√©e", []) if return_debug else "th√©matique non pr√©cis√©e"

    best_theme = max(scores, key=scores.get)
    return (best_theme, matched.get(best_theme, [])) if return_debug else best_theme

# -----------------------
# Pr√©paration du DataFrame
# -----------------------
def prepare_dataframe(df: pd.DataFrame) -> pd.DataFrame:
    df = df.copy()

    # Arrondissement
    if "arrondissement_projet_gagnant" in df.columns:
        df["arr_num"] = df["arrondissement_projet_gagnant"].apply(normalize_arrondissement_from_text)
    else:
        df["arr_num"] = None

    # Avancement
    def map_status_db(v):
        txt = normalize_text(v)
        try:
            for k, val in STATUS_MAP.items():  # STATUS_MAP d√©clar√© ailleurs
                if k in txt:
                    return val
        except NameError:
            pass
        return v.upper() if isinstance(v, str) else v

    if "avancement_projet" in df.columns:
        df["avancement_norm"] = df["avancement_projet"].apply(map_status_db)
    else:
        df["avancement_norm"] = None

    # Th√©matique
    if "thematique" in df.columns:
        df["thematique_norm"] = df["thematique"].apply(map_thematique_free_text)
    else:
        df["thematique_norm"] = "th√©matique non pr√©cis√©e"

    # Budget num√©rique
    if "budget_global_projet_gagnant" in df.columns:
        df["budget_num"] = df["budget_global_projet_gagnant"].apply(to_float_budget)
    else:
        df["budget_num"] = np.nan

    # Texte combin√© pour scoring simple
    df["text_all"] = (
        df.get("titre_projet_gagnant", pd.Series(dtype=str)).fillna("") + " " +
        df.get("thematique_norm", pd.Series(dtype=str)).fillna("") + " " +
        df.get("avancement_norm", pd.Series(dtype=str)).fillna("")
    )
    return df

def get_nearby_arrondissements(arr: Optional[int]) -> list:
    if arr is None:
        return []
    nearby = []
    if arr > 1:
        nearby.append(arr - 1)
    nearby.append(arr)
    if arr < 20:
        nearby.append(arr + 1)
    return nearby

# -----------------------
# Filtrage FIN + ABANDONN√â
# -----------------------
def filter_fin_abandoned(df: pd.DataFrame, description: str) -> Tuple[pd.DataFrame, pd.DataFrame, Optional[int], str, Optional[float]]:
    arr_user = normalize_arrondissement_from_text(description)
    theme_user = map_thematique_free_text(description)
    user_budget = parse_budget_from_text(description)

    base_mask = (df["arr_num"] == arr_user) & (df["thematique_norm"] == theme_user)

    df_fin = df[base_mask & (df["avancement_norm"] == "FIN")].copy()
    df_ab = df[base_mask & (df["avancement_norm"].isin(["ABANDON", "ABANDONN√â", "ABANDONNE"]))].copy()

    return df_fin, df_ab, arr_user, theme_user, user_budget

def filter_projects_with_abandoned(df: pd.DataFrame, description: str):
    return filter_fin_abandoned(df, description)

# -----------------------
# Scoring candidats
# -----------------------
def score_candidates(df_candidates: pd.DataFrame, description: str, user_budget: Optional[float], w_text: float = 0.6, w_budget: float = 0.4) -> pd.DataFrame:
    if df_candidates is None or len(df_candidates) == 0:
        return df_candidates
    desc_norm = normalize_text(description)
    try:
        kws = list(THEMATIQUE_MAP.keys())  # THEMATIQUE_MAP d√©clar√© ailleurs
    except NameError:
        kws = []

    def text_score(row):
        txt = normalize_text(row.get("text_all", ""))
        hits = sum(1 for kw in kws if kw in desc_norm and kw in txt)
        return float(hits)

    def budget_score(row):
        b = row.get("budget_num", np.nan)
        if user_budget is None or pd.isna(b):
            return 0.0
        return 1.0 / (1.0 + abs(b - user_budget) / (user_budget + 1e-6))

    scored = df_candidates.copy()
    scored["score_text"] = scored.apply(text_score, axis=1)
    scored["score_budget"] = scored.apply(budget_score, axis=1)
    scored["score"] = w_text * scored["score_text"] + w_budget * scored["score_budget"]
    return scored.sort_values(by="score", ascending=False)

# -----------------------
# Intent & r√©ponses sociales
# -----------------------

from typing import Iterable, Tuple, Optional

# RapidFuzz is recommended for typo tolerance.
# pip install rapidfuzz
from rapidfuzz import process, fuzz


# Thresholds
SHORT_WORD_THRESHOLD = 75
WORD_THRESHOLD = 80
THEME_THRESHOLD = 70

# Dictionnaire de r√©ponses sociales (personnalis√©es)
SOCIAL_RESPONSES = {
    "bonjour": [
        "Bonjour üëã ‚Äî Ravi de vous voir, parlez-moi de votre projet.",
        "Salut üëã ‚Äî Content de vous retrouver, quelle est votre id√©e aujourd‚Äôhui ?"
    ],
    "salut": [
        "Salut üôÇ ‚Äî Dites-moi votre projet, je vous propose des exemples similaires.",
        "Salut üëã ‚Äî Quelle initiative souhaitez-vous explorer ?"
    ],
    "merci": [
        "Avec plaisir üôè ‚Äî Je suis l√† pour vous aider √† trouver des projets similaires.",
        "Merci √† vous ü§ù ‚Äî Voulez-vous que je vous propose des recommandations ?"
    ],
    "au revoir": [
        "Au revoir üëã ‚Äî √Ä bient√¥t pour de nouvelles id√©es citoyennes.",
        "Bonne journ√©e üåû ‚Äî Revenez quand vous voulez proposer un projet."
    ],
    "√ßa va": [
        "√áa va tr√®s bien üôÇ ‚Äî Et vous ? Pr√™t √† partager une id√©e de projet ?",
        "Je vais bien merci üôå ‚Äî Parlez-moi de votre projet pour que je vous propose des suggestions."
    ],
    "bonsoir": [
        "Bonsoir üåô ‚Äî Une id√©e de projet pour ce soir ?",
        "Bonsoir üëã ‚Äî Je peux vous montrer des projets similaires d√©j√† r√©alis√©s."
    ]
}

# Liste des mots-cl√©s sociaux
GREETINGS = list(SOCIAL_RESPONSES.keys())

def _normalize_choices(choices: Iterable[str]) -> list:
    return [normalize_text(c) for c in choices]

def fuzzy_contains_any(text: str, choices: Iterable[str],
                       scorer=fuzz.token_sort_ratio, threshold: int = WORD_THRESHOLD
                       ) -> Tuple[Optional[str], int]:
    if not text or not choices:
        return None, 0
    txt_norm = normalize_text(text)
    choices_list = list(choices)
    choices_norm = _normalize_choices(choices_list)
    match = process.extractOne(txt_norm, choices_norm, scorer=scorer)
    if not match:
        return None, 0
    best_norm, score, idx = match
    original_best = choices_list[idx] if 0 <= idx < len(choices_list) else best_norm
    return (original_best, int(score)) if score >= threshold else (None, int(score))

def quick_social_detect(message: str) -> Optional[str]:
    """Retourne le type de salutation d√©tect√©e (ex: 'bonjour', 'merci')"""
    match, score = fuzzy_contains_any(message, GREETINGS, scorer=fuzz.partial_ratio, threshold=SHORT_WORD_THRESHOLD)
    return match if match else None

def handle_social(message: str) -> str:
    """Retourne une r√©ponse adapt√©e selon la salutation d√©tect√©e"""
    greeting = quick_social_detect(message)
    if greeting and greeting in SOCIAL_RESPONSES:
        return random.choice(SOCIAL_RESPONSES[greeting])
    # fallback si pas trouv√©
    return "Bonjour üëã ‚Äî Parlez-moi de votre projet, je vous propose des projets similaires."

def detect_intent(message: str) -> str:
    """
    Intent detection:
    1) fuzzy social check,
    2) fuzzy 'projet' or thematic keys,
    3) fallback LLM (keeps previous behavior).
    """
    if quick_social_detect(message):
        return "social"

    # Pr√©parer them_keys si disponible
    try:
        them_keys = list(THEMATIQUE_MAP.keys())
    except Exception:
        them_keys = []

    # Fuzzy check for 'projet'
    proj_match, proj_score = fuzzy_contains_any(message, ["projet"], scorer=fuzz.partial_ratio, threshold=WORD_THRESHOLD)
    if proj_match:
        return "project"

    # Fuzzy check for thematic keys
    if them_keys:
        match, score = fuzzy_contains_any(message, them_keys, scorer=fuzz.token_set_ratio, threshold=THEME_THRESHOLD)
        if match:
            return "project"

    # Fallback LLM minimal
    prompt = f'Reponds uniquement par "social" ou "project". Message: "{message}"'
    try:
        reply = ollama_generate(prompt, max_tokens=30, timeout_s=30.0)
        r = (reply or "").lower().strip()
        if "project" in r:
            return "project"
        if "social" in r:
            return "social"
    except Exception:
        pass

    return "project"

# -----------------------
# LLM: appels optimis√©s
# -----------------------

def _normalize_bullets(raw: str, max_lines: int = 3, pad: bool = True) -> str:
    """
    Nettoie la sortie LLM :
    - supprime pr√©fixes multiples (ex: "- -", "1. -", etc.)
    - assure un pr√©fixe unique "- " par ligne
    - retire les lignes placeholder renvoy√©es par le LLM (ex: "Suggestion suppl√©mentaire")
    - d√©duplique les lignes en conservant l'ordre
    - limite √† max_lines
    - si pad==True, compl√®te avec des lignes '- Suggestion suppl√©mentaire' si < max_lines
    """
    if not raw:
        return "\n".join(["- Suggestion suppl√©mentaire"] * max_lines) if pad else ""
    # Normalisation des lignes
    lines = [l.strip() for l in raw.splitlines() if l.strip()]
    cleaned = []
    for l in lines:
        # Supprime num√©rotation et tirets en d√©but de ligne
        l = re.sub(r"^\s*(?:\d+[.)]\s*)?[-\s]+", "", l)
        # Compresse espaces internes
        l = re.sub(r"\s+", " ", l).strip()
        if not l:
            continue
        # Filtre les placeholders √©mis par le mod√®le (variantes et casse)
        if re.fullmatch(r"(?i)suggestion\s+suppl[√©e]mentaire\.?", l):
            continue
        cleaned.append(l)
    # D√©duplication en conservant l'ordre
    seen = set()
    deduped = []
    for l in cleaned:
        key = l.lower()
        if key in seen:
            continue
        seen.add(key)
        deduped.append(l)
    # Pr√©fixe unique "- " et coupe √† max_lines
    deduped = [f"- {l}" for l in deduped]
    if len(deduped) > max_lines:
        deduped = deduped[:max_lines]
    # Padding uniquement si demand√© et n√©cessaire
    if pad:
        while len(deduped) < max_lines:
            deduped.append("- Suggestion suppl√©mentaire")
    return "\n".join(deduped)


def _normalize_bullets_streaming(raw: str, max_lines: int = 3) -> str:
    """
    Version all√©g√©e pour le streaming - ne casse pas le texte incomplet.
    - G√®re les lignes partielles sans les filtrer
    - Pas de padding (√©vite d'afficher des placeholders pendant le streaming)
    - Normalisation minimale pour garder le texte lisible en temps r√©el
    """
    if not raw:
        return ""
    
    # Enlever les blocs de code markdown si pr√©sents
    raw = re.sub(r"```.*?```", "", raw, flags=re.DOTALL)
    raw = raw.strip()
    
    if not raw:
        return ""
    
    # Split par lignes mais garde les lignes incompl√®tes
    lines = raw.split('\n')
    cleaned = []
    
    for i, l in enumerate(lines):
        l = l.strip()
        if not l:
            continue
        
        # Supprime les pr√©fixes multiples mais garde le texte partiel
        l_clean = re.sub(r"^\s*(?:\d+[.)]\s*)?[-‚Ä¢*\s]+", "", l).strip()
        
        # Si la ligne est vide apr√®s nettoyage, skip
        if not l_clean:
            continue
        
        # Ne filtre PAS les placeholders pendant le streaming
        # (ils peuvent √™tre du texte partiel en cours de g√©n√©ration)
        
        # Ajoute le pr√©fixe si ce n'est pas d√©j√† une bullet
        if not l.startswith(('-', '‚Ä¢', '*')):
            l_clean = f"- {l_clean}"
        else:
            l_clean = f"- {l_clean}"
        
        cleaned.append(l_clean)
        
        # Limite souple pendant le streaming (permet de voir plus de contenu)
        if len(cleaned) >= max_lines + 1:  # +1 pour voir une ligne en cours
            break
    
    # D√©duplication l√©g√®re seulement sur les lignes compl√®tes
    # (garde les duplicatas potentiels si c'est du texte en cours)
    if len(cleaned) > max_lines:
        seen = set()
        deduped = []
        for l in cleaned[:max_lines]:
            key = l.lower()
            if key not in seen:
                seen.add(key)
                deduped.append(l)
        cleaned = deduped
    
    return '\n'.join(cleaned)
def ollama_generate(prompt: str, max_tokens: int = 80, timeout_s: float = 30.0) -> str:
    """
    Appel rapide √† Ollama : stream=False, tokens limit√©s, timeout court.
    Retourne 3 puces max, avec fallback en cas d‚Äôerreur.
    """
    payload = {
        "model": DEFAULT_MODEL,
        "prompt": prompt,
        "stream": False,
        "options": {
            "num_predict": max_tokens,
            "temperature": 0.15,
            "top_p": 0.9,
            "stop": ["\n\n"]
        },
    }
    try:
        resp = requests.post(OLLAMA_URL, json=payload, timeout=timeout_s)
        resp.raise_for_status()
        data = resp.json()
        text = (data.get("response") or "").strip()
        if not text:
            return "- Aucune suggestion\n- R√©duisez le p√©rim√®tre ou pr√©cisez la th√©matique\n- R√©essayez"
        return _normalize_bullets(text, max_lines=3)
    except Exception as e:
        return f"- Erreur LLM: {e}\n- Essayez encore\n- Ou simplifiez la requ√™te"

def ollama_stream_generate_chunks(prompt: str, max_tokens: int = 80, timeout_s: float = 60.0) -> Generator[str, None, None]:
    """
    Streaming propre: renvoie uniquement le champ 'response' de chaque chunk.
    """
    payload = {
        "model": DEFAULT_MODEL,
        "prompt": prompt,
        "stream": True,
        "options": {
            "num_predict": max_tokens,
            "temperature": 0.15,
            "top_p": 0.9,
            "stop": ["\n\n"]
        },
    }
    with requests.post(OLLAMA_URL, json=payload, stream=True, timeout=timeout_s) as r:
        r.raise_for_status()
        for line in r.iter_lines():
            if not line:
                continue
            try:
                data = json.loads(line.decode("utf-8"))
            except Exception:
                continue
            if "response" in data and data["response"]:
                yield data["response"]
            if data.get("done"):
                break

def warm_up_model_async():
    def _warmup():
        try:
            _ = ollama_generate("Bonjour", max_tokens=16, timeout_s=30.0)
        except Exception:
            pass
    threading.Thread(target=_warmup, daemon=True).start()

# -----------------------
# Prompts de suggestions (orient√©s projets concrets)
# -----------------------
def build_fast_suggestions_prompt(description: str, df_fin: pd.DataFrame, df_ab: pd.DataFrame, max_items: int = 3) -> str:
    """
    Prompt compact et cibl√©: fournit des r√©f√©rences mais exige 3 projets NEUFS.
    """
    arr = normalize_arrondissement_from_text(description) or "non pr√©cis√©"
    budget = parse_budget_from_text(description) or "non pr√©cis√©"
    theme = map_thematique_free_text(description)

    def row_line(r):
        title = r.get("titre_projet_gagnant", "‚Äî")
        bud = r.get("budget_global_projet_gagnant", "‚Äî")
        edi = r.get("edition", "‚Äî")
        return f"{title} (budget {bud} ‚Ç¨, √©dition {edi})"

    lines = [
        f"Contrainte: arrondissement {arr}, th√©matique {theme}, budget max {budget} ‚Ç¨.",
        "R√©f√©rence ‚Äî projets finis (ne pas r√©utiliser, uniquement contexte):"
    ]
    if df_fin is not None and len(df_fin) > 0:
        for _, r in df_fin.head(max_items).iterrows():
            lines.append(f"- {row_line(r)}")
    else:
        lines.append("- Aucun")

    lines.append("R√©f√©rence ‚Äî projets abandonn√©s (ne pas r√©utiliser, uniquement contexte):")
    if df_ab is not None and len(df_ab) > 0:
        for _, r in df_ab.head(max_items - 1).iterrows():
            lines.append(f"- {row_line(r)}")
    else:
        lines.append("- Aucun")

    instr = (
    "Propose EXACTEMENT 3 id√©es de projets citoyens PERMANENTS et INFRASTRUCTUR√âS, jamais des √©v√©nements ponctuels. "
    "Contraintes strictes : "
    "- Chaque id√©e doit tenir sur une seule ligne et commencer par \"- \". "
    "- Interdiction absolue de proposer des √©v√©nements, journ√©es, campagnes ponctuelles, ateliers uniques, conf√©rences ou manifestations. "
    "- Chaque id√©e doit d√©crire un √©l√©ment physique ou un service p√©renne (ex : am√©nagement, √©quipement, installation, local transform√©, infrastructure verte, service de pr√™t, programme permanent). "
    "- Fournir des √©l√©ments concrets : emplacement type ou surface, et un indicateur de r√©sultat ou d'usage mesurable (ex : m¬≤, %, nombre d'usagers). "
    "- Ne pas citer ni reformuler les projets finis ou abandonn√©s fournis en r√©f√©rence. "
    "- Rester factuel, pr√©cis et r√©alisable ; √©viter le vague et les formulations √©v√©nementielles. "
    "Donne EXACTEMENT 3 lignes conformes."
)


    return instr + "\n\n" + "\n".join(lines)

# -----------------------
# Cl√© cache pour suggestions
# -----------------------
def _suggestions_cache_key(description: str, df_fin: pd.DataFrame, df_ab: pd.DataFrame) -> str:
    def digest_df(df: pd.DataFrame, n: int) -> str:
        if df is None or len(df) == 0:
            return "empty"
        rows = []
        for _, r in df.head(n).iterrows():
            rows.append(f"{r.get('titre_projet_gagnant','')}|{r.get('budget_global_projet_gagnant','')}|{r.get('edition','')}")
        return ";".join(rows)
    raw = f"{description}||FIN:{digest_df(df_fin, 3)}||AB:{digest_df(df_ab, 2)}"
    return hashlib.sha1(raw.encode("utf-8")).hexdigest()

# -----------------------
# Suggestions LLM (direct) et streaming lettre par lettre
# -----------------------
def generate_project_suggestions(description: str, df_fin_scored: pd.DataFrame, df_ab_scored: pd.DataFrame, streaming: bool = False) -> str:
    """
    Non-stream ‚Äî retourne 3 puces format√©es. Cache en m√©moire.
    """
    key = _suggestions_cache_key(description, df_fin_scored, df_ab_scored)
    cached = _SUGGESTIONS_CACHE.get(key)
    if cached:
        return cached

    prompt = build_fast_suggestions_prompt(description, df_fin_scored.head(3), df_ab_scored.head(2), max_items=3)
    text = ollama_generate(prompt, max_tokens=250, timeout_s=60.0)
    _SUGGESTIONS_CACHE[key] = text
    return text

def stream_project_suggestions_letters(description: str, df_fin_scored: pd.DataFrame, df_ab_scored: pd.DataFrame) -> Generator[str, None, None]:
    """
    Stream incremental chunks character-by-character for smooth letter-by-letter display.
    - Yields accumulated text after each chunk (creates streaming effect).
    - On completion, yields the final normalized text and caches it.
    """
    key = _suggestions_cache_key(description, df_fin_scored, df_ab_scored)
    cached = _SUGGESTIONS_CACHE.get(key)
    if cached:
        # For cached content, simulate streaming effect
        for i in range(1, len(cached) + 1):
            yield cached[:i]
            time.sleep(0.01)  # Small delay for visual effect
        return
    
    prompt = build_fast_suggestions_prompt(description, df_fin_scored.head(3), df_ab_scored.head(2), max_items=3)
    collected = []
    
    try:
        for chunk in ollama_stream_generate_chunks(prompt, max_tokens=250, timeout_s=45.0):
            if not chunk:
                continue
            collected.append(chunk)
            
            # Yield accumulated text after EVERY chunk (creates letter-by-letter effect)
            partial_raw = "".join(collected)
            # Apply light normalization that preserves partial text
            partial = _normalize_bullets_streaming(partial_raw, max_lines=3)
            yield partial
            
    except Exception:
        # Fallback synchronous if streaming fails
        text = generate_project_suggestions(description, df_fin_scored, df_ab_scored)
        text = _normalize_bullets(text, max_lines=3)
        _SUGGESTIONS_CACHE[key] = text
        yield text
        return
    
    # Final normalization and cache
    final_raw = "".join(collected).strip()
    if not final_raw:
        text = generate_project_suggestions(description, df_fin_scored, df_ab_scored)
        text = _normalize_bullets(text, max_lines=3)
    else:
        text = _normalize_bullets(final_raw, max_lines=3)
    
    _SUGGESTIONS_CACHE[key] = text
    # Yield final normalized version
    yield text



# -----------------------
# Effet machine √† √©crire
# -----------------------
def _typewriter(text: str, delay_s: float = 0.01, chunk_size: int = 1) -> Generator[str, None, None]:
    if chunk_size <= 1:
        for ch in text:
            yield ch
            if delay_s > 0:
                time.sleep(delay_s)
    else:
        for i in range(0, len(text), chunk_size):
            yield text[i:i+chunk_size]
            if delay_s > 0:
                time.sleep(delay_s)